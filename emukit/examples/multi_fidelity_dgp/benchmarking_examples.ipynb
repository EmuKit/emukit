{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Fidelity Deep Gaussian process benchmark\n",
    "\n",
    "This notebook replicates the benchmark experiments from the paper:\n",
    "\n",
    "[Deep Gaussian Processes for Multi-fidelity Modeling (Kurt Cutajar, Mark Pullin, Andreas Damianou, Neil Lawrence, Javier Gonz√°lez)](https://arxiv.org/abs/1903.07320)\n",
    "\n",
    "Note that the code for the \"Deep Multi-fidelity Gaussian process\" is not publically available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import emukit.examples.multi_fidelity_dgp\n",
    "\n",
    "from emukit.examples.multi_fidelity_dgp.baseline_model_wrappers import Ar1Model, Nargp, HfGpOnly\n",
    "\n",
    "from emukit.core import ContinuousParameter, ParameterSpace\n",
    "from emukit.experimental_design import LatinDesign, RandomDesign\n",
    "from emukit.multi_fidelity.models import MultiFidelityDeepGP\n",
    "\n",
    "from emukit.test_functions.multi_fidelity import (multi_fidelity_borehole_function, multi_fidelity_branin_function,\n",
    "                                                  multi_fidelity_park_function, multi_fidelity_hartmann_3d,\n",
    "                                                  multi_fidelity_currin_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for different benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcn_names = ['currin', 'borehole', 'park', 'branin', 'hartmann-3d']\n",
    "\n",
    "fcn_name = 'currin'\n",
    "\n",
    "# Parameters for different benchmark functions\n",
    "y_scale_dict = {'borehole': 100, 'branin': 1, 'currin': 1, 'park': 1, 'hartmann-3d': 1}\n",
    "\n",
    "noise_level_dict = {'borehole': [0.05, 0.1], 'branin': [0, 0, 0], 'currin': [0, 0], \n",
    "                    'park': [0, 0], 'hartmann-3d': [0, 0, 0]}\n",
    "\n",
    "do_x_scaling_dict = {'borehole': True, 'branin': False, 'currin': False, 'park': False, 'hartmann-3d': False}\n",
    "\n",
    "num_data_dict = {'borehole': [60, 5], 'branin': [80, 30, 10], 'currin': [12, 5], \n",
    "                    'park': [30, 5], 'hartmann-3d': [80, 40, 20]}\n",
    "\n",
    "\n",
    "\n",
    "fcns = {'borehole': multi_fidelity_borehole_function, 'branin': multi_fidelity_branin_function,\n",
    "       'park': multi_fidelity_park_function, 'currin': multi_fidelity_currin_function, \n",
    "       'hartmann-3d': multi_fidelity_hartmann_3d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to repeat test across different random seeds.\n",
    "\n",
    "def do_benchmark(fcn_name):\n",
    "    metrics = dict()\n",
    "    fcn, space, n_data = get_fcn_params(fcn_name)\n",
    "\n",
    "    # Some random seeds to use\n",
    "    seeds = [123, 184, 202, 289, 732]\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "        run_name = str(seed) + str(n_data)\n",
    "        metrics[run_name] = test_function(fcn, space, n_data, seed)\n",
    "        print('After ' + str(i+1) + ' runs of ' + fcn_name)\n",
    "        print_metrics(metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics as table \n",
    "def print_metrics(metrics):\n",
    "    model_names = list(list(metrics.values())[0].keys())\n",
    "    metric_names = ['r2', 'mnll', 'rmse']\n",
    "    table = PrettyTable(['model'] + metric_names)\n",
    "\n",
    "    for name in model_names:\n",
    "        mean = []\n",
    "        for metric_name in metric_names:\n",
    "            mean.append(np.mean([metric[name][metric_name] for metric in metrics.values()]))\n",
    "        table.add_row([name] + mean)\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters for a given function\n",
    "def get_fcn_params(fcn_name):\n",
    "    handles = fcns[fcn_name]()[0].f\n",
    "    space = fcns[fcn_name]()[1]\n",
    "    n_data = num_data_dict[fcn_name]\n",
    "    \n",
    "    space._parameters = space._parameters[:-1]\n",
    "    return handles, space, n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(fcn, space, n_data, seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    x_test, y_test, X, Y = generate_data(fcn, n_data, 1000, space)\n",
    "\n",
    "    mf_dgp_fix_lf_mean = MultiFidelityDeepGP(X, Y, n_iter=5000)\n",
    "    mf_dgp_fix_lf_mean.name = 'mf_dgp_fix_lf_mean'\n",
    "\n",
    "    models = [HfGpOnly(X, Y), Ar1Model(X, Y), Nargp(X, Y), mf_dgp_fix_lf_mean]\n",
    "    return benchmark_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_models(models, x_test, y_test):\n",
    "    metrics = dict()\n",
    "    for model in models:\n",
    "        model.optimize()\n",
    "        y_mean, y_var = model.predict(x_test)\n",
    "        metrics[model.name] = calculate_metrics(y_test, y_mean, y_var)\n",
    "        print('+ ######################## +')\n",
    "        print(model.name, 'r2', metrics[model.name]['r2'])\n",
    "        print('+ ######################## + ')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(fcn, n_data, n_test_points, space):\n",
    "    \"\"\"\n",
    "    Generates train and test data for\n",
    "    \"\"\"\n",
    "    \n",
    "    do_x_scaling = do_x_scaling_dict[fcn_name]\n",
    "    \n",
    "    # Generate training data\n",
    "    latin = LatinDesign(space)\n",
    "    X = [latin.get_samples(n) for n in n_data]\n",
    "    \n",
    "    # Scale X if required\n",
    "    if do_x_scaling:\n",
    "        scalings = X[0].std(axis=0)\n",
    "    else:\n",
    "        scalings = np.ones(X[0].shape[1])\n",
    "        \n",
    "    for x in X:\n",
    "        x /= scalings\n",
    "    \n",
    "    Y = []\n",
    "    for i, x in enumerate(X):\n",
    "        Y.append(fcn[i](x * scalings))\n",
    "    \n",
    "    y_scale = y_scale_dict[fcn_name]\n",
    "    \n",
    "    # scale y and add noise if required\n",
    "    noise_levels = noise_level_dict[fcn_name]\n",
    "    for y, std_noise in zip(Y, noise_levels):\n",
    "        y /= y_scale + std_noise * np.random.randn(y.shape[0], 1)\n",
    "    \n",
    "    # Generate test data\n",
    "    x_test = latin.get_samples(n_test_points)\n",
    "    x_test /= scalings\n",
    "    y_test = fcn[-1](x_test * scalings)\n",
    "    y_test /= y_scale\n",
    "\n",
    "    i_highest_fidelity = (len(n_data) - 1) * np.ones((x_test.shape[0], 1))\n",
    "    x_test = np.concatenate([x_test, i_highest_fidelity], axis=1)\n",
    "    print(X[1].shape)\n",
    "    return x_test, y_test, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_mean_prediction, y_var_prediction):\n",
    "    # R2\n",
    "    r2 = r2_score(y_test, y_mean_prediction)\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_mean_prediction))\n",
    "    # Test log likelihood\n",
    "    mnll = -np.sum(scipy.stats.norm.logpdf(y_test, loc=y_mean_prediction, scale=np.sqrt(y_var_prediction)))/len(y_test)\n",
    "    return {'r2': r2, 'rmse': rmse, 'mnll': mnll}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = []\n",
    "metrics.append(do_benchmark(fcn_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (metric) in zip(metrics):\n",
    "    print(fcn_name)\n",
    "    print_metrics(metric[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
